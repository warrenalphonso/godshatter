{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tanh` vs `ReLU` in a small CNN\n",
    "\n",
    "Section 3.1 in the [AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "paper claims that `tanh` takes much longer to reach 25% accuracy on the training\n",
    "data than `ReLU`. [PDLT](https://arxiv.org/abs/2106.10165) shows that `tanh` can\n",
    "reach criticality if it's initialized correctly. \n",
    "\n",
    "Let's test this! I want to use a really small network, so how about a two layer\n",
    "CNN with 1 linear layer on MNIST?\n",
    "\n",
    "Here's Figure 1 in the AlexNet paper: \n",
    "\n",
    "![](../assets/alexnet-fig1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.MNIST(\n",
    "    root=\"../datasets\",\n",
    "    # ToTensor scales [0,255] to [0.0, 1.0]\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0 - zero': 0,\n",
       " '1 - one': 1,\n",
       " '2 - two': 2,\n",
       " '3 - three': 3,\n",
       " '4 - four': 4,\n",
       " '5 - five': 5,\n",
       " '6 - six': 6,\n",
       " '7 - seven': 7,\n",
       " '8 - eight': 8,\n",
       " '9 - nine': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourLayerCNN(nn.Module):\n",
    "    \"\"\"4 Conv2Ds in a row with max pooling. Then\"\"\"\n",
    "\n",
    "    def __init__(self, activation_name: Literal[\"relu\", \"tanh\"], n_classes=100):\n",
    "        activation = nn.ReLU if activation_name == \"relu\" else nn.Tanh\n",
    "        super().__init__()\n",
    "        # MNIST is (28,28,1)\n",
    "        self.layers = nn.Sequential(\n",
    "            # (b, 1, 28, 28) -> (b, 32, 28, 28)\n",
    "            nn.Conv2d(1, 32, (3, 2), stride=1, padding=\"same\"),\n",
    "            activation(),\n",
    "            # (b, 32, 28, 28) -> (b, 32, 14, 14)\n",
    "            nn.MaxPool2d((2, 2), stride=2),\n",
    "            # (b, 32, 14, 14) -> (b, 64, 14, 14)\n",
    "            nn.Conv2d(32, 64, (3, 2), stride=1, padding=\"same\"),\n",
    "            activation(),\n",
    "            # (b, 64, 14, 14) -> (b, 64, 7, 7)\n",
    "            nn.MaxPool2d((2, 2), stride=2),\n",
    "            # (b, 64, 7, 7) -> (b, 3136)\n",
    "            nn.Flatten(),\n",
    "            # (b, 3136) -> (b, 128)\n",
    "            nn.Linear(3136, 128),\n",
    "            activation(),\n",
    "            nn.Linear(128, n_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        # Initialization based on PDLT conditions for criticality\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                if activation_name == \"relu\":\n",
    "                    nn.init.normal(\n",
    "                        layer.weight,\n",
    "                        mean=0,\n",
    "                        std=(2 / (layer.kernel_size[0] * layer.kernel_size[1])) ** (1 / 2),\n",
    "                    )\n",
    "                    nn.init.constant(layer.bias, 0)\n",
    "                else:\n",
    "                    nn.init.normal(\n",
    "                        layer.weight,\n",
    "                        mean=0,\n",
    "                        std=(1 / (layer.kernel_size[0] * layer.kernel_size[1])) ** (1 / 2),\n",
    "                    )\n",
    "                    nn.init.constant(layer.bias, 0)\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                if activation_name == \"relu\":\n",
    "                    nn.init.normal(layer.weight, mean=0, std=(2 / layer.out_features) ** (1 / 2))\n",
    "                    nn.init.constant(layer.bias, 0)\n",
    "                else:\n",
    "                    nn.init.normal(layer.weight, mean=0, std=(1 / layer.out_features) ** (1 / 2))\n",
    "                    nn.init.constant(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
