{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d52c6bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used:  1.03 GB, h.1.ln_1.bias                                     :  11%| | "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte.weight (50257, 768)\n",
      "wpe.weight (1024, 768)\n",
      "h.0.ln_1.weight (768,)\n",
      "h.0.ln_1.bias (768,)\n",
      "h.0.attn.c_attn.weight (2304, 768)\n",
      "h.0.attn.c_attn.bias (2304,)\n",
      "h.0.attn.c_proj.weight (768, 768)\n",
      "h.0.attn.c_proj.bias (768,)\n",
      "h.0.ln_2.weight (768,)\n",
      "h.0.ln_2.bias (768,)\n",
      "h.0.mlp.c_fc.weight (3072, 768)\n",
      "h.0.mlp.c_fc.bias (3072,)\n",
      "h.0.mlp.c_proj.weight (768, 3072)\n",
      "h.0.mlp.c_proj.bias (768,)\n",
      "h.1.ln_1.weight (768,)\n",
      "h.1.ln_1.bias (768,)\n",
      "h.1.attn.c_attn.weight (2304, 768)\n",
      "h.1.attn.c_attn.bias (2304,)\n",
      "h.1.attn.c_proj.weight (768, 768)\n",
      "h.1.attn.c_proj.bias (768,)\n",
      "h.1.ln_2.weight (768,)\n",
      "h.1.ln_2.bias (768,)\n",
      "h.1.mlp.c_fc.weight (3072, 768)\n",
      "h.1.mlp.c_fc.bias (3072,)\n",
      "h.1.mlp.c_proj.weight (768, 3072)\n",
      "h.1.mlp.c_proj.bias (768,)\n",
      "h.2.ln_1.weight (768,)\n",
      "h.2.ln_1.bias (768,)\n",
      "h.2.attn.c_attn.weight (2304, 768)\n",
      "h.2.attn.c_attn.bias (2304,)\n",
      "h.2.attn.c_proj.weight (768, 768)\n",
      "h.2.attn.c_proj.bias (768,)\n",
      "h.2.ln_2.weight (768,)\n",
      "h.2.ln_2.bias (768,)\n",
      "h.2.mlp.c_fc.weight (3072, 768)\n",
      "h.2.mlp.c_fc.bias (3072,)\n",
      "h.2.mlp.c_proj.weight (768, 3072)\n",
      "h.2.mlp.c_proj.bias (768,)\n",
      "h.3.ln_1.weight (768,)\n",
      "h.3.ln_1.bias (768,)\n",
      "h.3.attn.c_attn.weight (2304, 768)\n",
      "h.3.attn.c_attn.bias (2304,)\n",
      "h.3.attn.c_proj.weight (768, 768)\n",
      "h.3.attn.c_proj.bias (768,)\n",
      "h.3.ln_2.weight (768,)\n",
      "h.3.ln_2.bias (768,)\n",
      "h.3.mlp.c_fc.weight (3072, 768)\n",
      "h.3.mlp.c_fc.bias (3072,)\n",
      "h.3.mlp.c_proj.weight (768, 3072)\n",
      "h.3.mlp.c_proj.bias (768,)\n",
      "h.4.ln_1.weight (768,)\n",
      "h.4.ln_1.bias (768,)\n",
      "h.4.attn.c_attn.weight (2304, 768)\n",
      "h.4.attn.c_attn.bias (2304,)\n",
      "h.4.attn.c_proj.weight (768, 768)\n",
      "h.4.attn.c_proj.bias (768,)\n",
      "h.4.ln_2.weight (768,)\n",
      "h.4.ln_2.bias (768,)\n",
      "h.4.mlp.c_fc.weight (3072, 768)\n",
      "h.4.mlp.c_fc.bias (3072,)\n",
      "h.4.mlp.c_proj.weight (768, 3072)\n",
      "h.4.mlp.c_proj.bias (768,)\n",
      "h.5.ln_1.weight (768,)\n",
      "h.5.ln_1.bias (768,)\n",
      "h.5.attn.c_attn.weight (2304, 768)\n",
      "h.5.attn.c_attn.bias (2304,)\n",
      "h.5.attn.c_proj.weight (768, 768)\n",
      "h.5.attn.c_proj.bias (768,)\n",
      "h.5.ln_2.weight (768,)\n",
      "h.5.ln_2.bias (768,)\n",
      "h.5.mlp.c_fc.weight (3072, 768)\n",
      "h.5.mlp.c_fc.bias (3072,)\n",
      "h.5.mlp.c_proj.weight (768, 3072)\n",
      "h.5.mlp.c_proj.bias (768,)\n",
      "h.6.ln_1.weight (768,)\n",
      "h.6.ln_1.bias (768,)\n",
      "h.6.attn.c_attn.weight (2304, 768)\n",
      "h.6.attn.c_attn.bias (2304,)\n",
      "h.6.attn.c_proj.weight (768, 768)\n",
      "h.6.attn.c_proj.bias (768,)\n",
      "h.6.ln_2.weight (768,)\n",
      "h.6.ln_2.bias (768,)\n",
      "h.6.mlp.c_fc.weight (3072, 768)\n",
      "h.6.mlp.c_fc.bias (3072,)\n",
      "h.6.mlp.c_proj.weight (768, 3072)\n",
      "h.6.mlp.c_proj.bias (768,)\n",
      "h.7.ln_1.weight (768,)\n",
      "h.7.ln_1.bias (768,)\n",
      "h.7.attn.c_attn.weight (2304, 768)\n",
      "h.7.attn.c_attn.bias (2304,)\n",
      "h.7.attn.c_proj.weight (768, 768)\n",
      "h.7.attn.c_proj.bias (768,)\n",
      "h.7.ln_2.weight (768,)\n",
      "h.7.ln_2.bias (768,)\n",
      "h.7.mlp.c_fc.weight (3072, 768)\n",
      "h.7.mlp.c_fc.bias (3072,)\n",
      "h.7.mlp.c_proj.weight (768, 3072)\n",
      "h.7.mlp.c_proj.bias (768,)\n",
      "h.8.ln_1.weight (768,)\n",
      "h.8.ln_1.bias (768,)\n",
      "h.8.attn.c_attn.weight (2304, 768)\n",
      "h.8.attn.c_attn.bias (2304,)\n",
      "h.8.attn.c_proj.weight (768, 768)\n",
      "h.8.attn.c_proj.bias (768,)\n",
      "h.8.ln_2.weight (768,)\n",
      "h.8.ln_2.bias (768,)\n",
      "h.8.mlp.c_fc.weight (3072, 768)\n",
      "h.8.mlp.c_fc.bias (3072,)\n",
      "h.8.mlp.c_proj.weight (768, 3072)\n",
      "h.8.mlp.c_proj.bias (768,)\n",
      "h.9.ln_1.weight (768,)\n",
      "h.9.ln_1.bias (768,)\n",
      "h.9.attn.c_attn.weight (2304, 768)\n",
      "h.9.attn.c_attn.bias (2304,)\n",
      "h.9.attn.c_proj.weight (768, 768)\n",
      "h.9.attn.c_proj.bias (768,)\n",
      "h.9.ln_2.weight (768,)\n",
      "h.9.ln_2.bias (768,)\n",
      "h.9.mlp.c_fc.weight (3072, 768)\n",
      "h.9.mlp.c_fc.bias (3072,)\n",
      "h.9.mlp.c_proj.weight (768, 3072)\n",
      "h.9.mlp.c_proj.bias (768,)\n",
      "h.10.ln_1.weight (768,)\n",
      "h.10.ln_1.bias (768,)\n",
      "h.10.attn.c_attn.weight (2304, 768)\n",
      "h.10.attn.c_attn.bias (2304,)\n",
      "h.10.attn.c_proj.weight (768, 768)\n",
      "h.10.attn.c_proj.bias (768,)\n",
      "h.10.ln_2.weight (768,)\n",
      "h.10.ln_2.bias (768,)\n",
      "h.10.mlp.c_fc.weight (3072, 768)\n",
      "h.10.mlp.c_fc.bias (3072,)\n",
      "h.10.mlp.c_proj.weight (768, 3072)\n",
      "h.10.mlp.c_proj.bias (768,)\n",
      "h.11.ln_1.weight (768,)\n",
      "h.11.ln_1.bias (768,)\n",
      "h.11.attn.c_attn.weight (2304, 768)\n",
      "h.11.attn.c_attn.bias (2304,)\n",
      "h.11.attn.c_proj.weight (768, 768)\n",
      "h.11.attn.c_proj.bias (768,)\n",
      "h.11.ln_2.weight (768,)\n",
      "h.11.ln_2.bias (768,)\n",
      "h.11.mlp.c_fc.weight (3072, 768)\n",
      "h.11.mlp.c_fc.bias (3072,)\n",
      "h.11.mlp.c_proj.weight (768, 3072)\n",
      "h.11.mlp.c_proj.bias (768,)\n",
      "ln_f.weight (768,)\n",
      "ln_f.bias (768,)\n",
      "lm_head.weight (50257, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used:  1.03 GB, lm_head.weight                                    : 100%|█| \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 1390.22 ms, 0.00 GB loaded at 0.00 GB/s\n"
     ]
    }
   ],
   "source": [
    "from jaxtyping import Shaped, jaxtyped\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, set_seed\n",
    "set_seed(42)\n",
    "from tinygrad import nn\n",
    "from tinygrad.tensor import Tensor\n",
    "Tensor.manual_seed(42)\n",
    "from tinygrad.dtype import dtypes\n",
    "from tinygrad import TinyJit\n",
    "from typechecker import typechecker\n",
    "\n",
    "from nanogpt.model import Block, TinyConfig, GPT2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "model = GPT2.load_from_huggingface()\n",
    "# model(Tensor.ones((4, TinyConfig.sequence_length), dtype=dtypes.int)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93f00640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, not a app. I've never got lazy. No matter what manuals you have on your audience adaptable if\n",
      "> Hello, I'm a language model, that is how I met Yamatanagi.\" We sat down for lunch. As we left Cologne June 15th\n",
      "> Hello, I'm a language model, much like I use desktop editors. But this time, people writing on modern languages are having the whole process,\n",
      "> Hello, I'm a language model, and Brian Conway's Immortal Language, if you so please let me know. I'd love this story to eventually\n",
      "> Hello, I'm a language model, I'm writing about philosophy and self-concepts. When you write an award winning book of philosophy and there\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "x = Tensor(tokens, dtype=\"long\", device=\"metal\").unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    logits = model(x)\n",
    "    assert logits.shape == (*x.shape, 50257)\n",
    "    logits = logits[:, -1, :]\n",
    "    assert logits.shape == (x.shape[0], 50257)\n",
    "    probs = logits.softmax(axis=-1)\n",
    "    assert probs.shape == (x.shape[0], 50257)\n",
    "\n",
    "    ix = probs.multinomial()\n",
    "    assert ix.shape == (x.shape[0], 1)\n",
    "    x = Tensor.cat(x, ix, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# while x.size(1) < max_length:\n",
    "\n",
    "\n",
    "\n",
    "# encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "# decode = lambda l: enc.decode(l)\n",
    "\n",
    "# start_ids = encode(\"Hello, I'm a language model,\")\n",
    "# idx = (Tensor(start_ids)[None, ...])\n",
    "# max_new_tokens = 16\n",
    "# temp = 1.0\n",
    "# top_k = 40\n",
    "\n",
    "# for _ in range(max_new_tokens):\n",
    "#     idx_cond = idx \n",
    "#     logits = model(idx_cond)\n",
    "#     logits = logits[:, -1, :] / temp\n",
    "#     idx_next = logits.softmax().multinomial()\n",
    "#     idx = Tensor.cat(idx, idx_next, dim=1)\n",
    "\n",
    "# decode(idx.tolist()[0])\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x.numpy()[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cc37c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, I'm a language model, and all this talk does……although it seems like there are lots of crazy special cases lurking nearby. Maybe there\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(x.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a62a007b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 50257)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "374e3932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ff426fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, which means I'm familiar with it, but I'm not fluent in that. Well, with that said,\n",
      "> Hello, I'm a language model, and the syntax, to make use of it, is pretty good. So why do you have that and not\n",
      "> Hello, I'm a language model, I'm doing this work in Python, and then I'm writing code for Haskell.\n",
      "\n",
      "So we can\n",
      "> Hello, I'm a language model, and you're making assumptions about my use of them. I'm not a natural language learner. I'm\n",
      "> Hello, I'm a language model, well, I'm from Java and have to write a programming language for it. I have my own vocabulary because\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124 M\n",
    "model_ = model_hf.to('mps')\n",
    "x = tokens.to(\"mps\")\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model_(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568f8d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wte.weight': <Tensor <UOp METAL (50257, 768) float ShapeTracker(views=(View(shape=(50257, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'wte.arange': <Tensor <UOp METAL (50257, 1) int ShapeTracker(views=(View(shape=(50257, 1), strides=(1, 0), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'wpe.weight': <Tensor <UOp METAL (1024, 768) float ShapeTracker(views=(View(shape=(1024, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'wpe.arange': <Tensor <UOp METAL (1024, 1) int ShapeTracker(views=(View(shape=(1024, 1), strides=(1, 0), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.0.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.0.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.0.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.0.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.0.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.0.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.0.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.0.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.0.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.0.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.0.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.0.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.1.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.1.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.1.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.1.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.1.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.1.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.1.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.1.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.1.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.1.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.1.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.1.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.2.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.2.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.2.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.2.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.2.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.2.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.2.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.2.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.2.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.2.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.2.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.2.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.3.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.3.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.3.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.3.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.3.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.3.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.3.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.3.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.3.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.3.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.3.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.3.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.4.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.4.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.4.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.4.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.4.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.4.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.4.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.4.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.4.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.4.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.4.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.4.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.5.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.5.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.5.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.5.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.5.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.5.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.5.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.5.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.5.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.5.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.5.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.5.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.6.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.6.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.6.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.6.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.6.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.6.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.6.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.6.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.6.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.6.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.6.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.6.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.7.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.7.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.7.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.7.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.7.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.7.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.7.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.7.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.7.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.7.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.7.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.7.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.8.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.8.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.8.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.8.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.8.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.8.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.8.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.8.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.8.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.8.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.8.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.8.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.9.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.9.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.9.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.9.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.9.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.9.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.9.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.9.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.9.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.9.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.9.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.9.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.10.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.10.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.10.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.10.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.10.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.10.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.10.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.10.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.10.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.10.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.10.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.10.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.11.ln_1.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.11.ln_1.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.11.attn.c_attn.weight': <Tensor <UOp METAL (2304, 768) float ShapeTracker(views=(View(shape=(2304, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.11.attn.c_attn.bias': <Tensor <UOp METAL (2304,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:2304 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.11.attn.c_proj.weight': <Tensor <UOp METAL (768, 768) float ShapeTracker(views=(View(shape=(768, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.11.attn.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.11.ln_2.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.11.ln_2.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.11.mlp.c_fc.weight': <Tensor <UOp METAL (3072, 768) float ShapeTracker(views=(View(shape=(3072, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.11.mlp.c_fc.bias': <Tensor <UOp METAL (3072,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:3072 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'h.11.mlp.c_proj.weight': <Tensor <UOp METAL (768, 3072) float ShapeTracker(views=(View(shape=(768, 3072), strides=(3072, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>,\n",
       " 'h.11.mlp.c_proj.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'ln_f.weight': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'ln_f.bias': <Tensor <UOp METAL (768,) float (<Ops.BUFFER: 11>, <buf real:True device:METAL size:768 dtype:dtypes.float>)> on METAL with grad None>,\n",
       " 'lm_head.weight': <Tensor <UOp METAL (50257, 768) float ShapeTracker(views=(View(shape=(50257, 768), strides=(768, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tinygrad.nn.state import get_state_dict\n",
    "\n",
    "get_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15674913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'transformer.wte.weight': torch.Size([50257, 768]), <class 'torch.Tensor'>\n",
      "'transformer.wpe.weight': torch.Size([1024, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.attn.c_attn.weight': torch.Size([768, 2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.mlp.c_fc.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.mlp.c_proj.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.ln_f.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.ln_f.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'lm_head.weight': torch.Size([50257, 768]), <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124 M\n",
    "sd_hf = model_hf.state_dict()\n",
    "for k, v in sd_hf.items():\n",
    "    print(f\"'{k}': {v.shape}, {type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd0f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The quick brown fox\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tok = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = tok[\"input_ids\"]\n",
    "model_hf = model_hf.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model_hf(input_ids=input_ids)\n",
    "    logits_hf = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0fd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'transformer.wte.weight': torch.Size([50257, 768]), <class 'torch.Tensor'>\n",
      "'transformer.wpe.weight': torch.Size([1024, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.0.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.1.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.2.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.3.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.4.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.5.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.6.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.7.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.8.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.9.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.10.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.ln_1.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.ln_1.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.attn.c_attn.weight': torch.Size([2304, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.attn.c_attn.bias': torch.Size([2304]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.attn.c_proj.weight': torch.Size([768, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.attn.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.ln_2.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.ln_2.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.mlp.c_fc.weight': torch.Size([3072, 768]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.mlp.c_fc.bias': torch.Size([3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.mlp.c_proj.weight': torch.Size([768, 3072]), <class 'torch.Tensor'>\n",
      "'transformer.h.11.mlp.c_proj.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.ln_f.weight': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'transformer.ln_f.bias': torch.Size([768]), <class 'torch.Tensor'>\n",
      "'lm_head.weight': torch.Size([50257, 768]), <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# model = GPT2.load_from_huggingface()\n",
    "# for k, v in model.state_dict().items():\n",
    "#     print(f\"'{k}': {v.shape}, {type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28b01ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-36.287136, -35.011105, -38.07908 , ..., -40.516106,\n",
       "         -41.37571 , -34.919064],\n",
       "        [-62.31387 , -61.56446 , -66.49383 , ..., -68.128494,\n",
       "         -68.32271 , -63.58289 ],\n",
       "        [-66.32398 , -66.74522 , -72.16191 , ..., -75.19552 ,\n",
       "         -73.46512 , -68.17866 ],\n",
       "        [-88.29095 , -88.723694, -93.442245, ..., -98.621155,\n",
       "         -90.63791 , -90.99138 ]]], shape=(1, 4, 50257), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "logits = model(tokens=Tensor(input_ids.numpy())).realize().numpy()\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99448cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'METAL'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tinygrad import Device\n",
    "\n",
    "Device.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ba2721",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     idx_next = logits.softmax().multinomial()\n\u001b[32m     17\u001b[39m     idx = Tensor.cat(idx, idx_next, dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m decoded = \u001b[43menc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(decoded)\n\u001b[32m     22\u001b[39m tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(\u001b[32m0\u001b[39m).repeat(num_return_sequences, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/godshatter/nanogpt/.venv/lib/python3.13/site-packages/tiktoken/core.py:284\u001b[39m, in \u001b[36mEncoding.decode\u001b[39m\u001b[34m(self, tokens, errors)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: Sequence[\u001b[38;5;28mint\u001b[39m], errors: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    273\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Decodes a list of tokens into a string.\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_core_bpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m, errors=errors)\n",
      "\u001b[31mTypeError\u001b[39m: argument 'tokens': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "start_ids = enc.encode(\"Hello, I'm a language model,\")\n",
    "x = (Tensor(start_ids)[None, ...])\n",
    "idx = x\n",
    "\n",
    "# for _ in range(max_length):\n",
    "#     idx_cond = idx\n",
    "#     logits = model(idx_cond)\n",
    "#     logits = logits[:, -1, :] / 1\n",
    "#     idx_next = logits.softmax().multinomial()\n",
    "#     idx = Tensor.cat(idx, idx_next, dim=1)\n",
    "\n",
    "# decoded = enc.decode(idx.tolist())\n",
    "# print(decoded)\n",
    "\n",
    "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "tokens = Tensor(tokens.numpy(), requires_grad=False)\n",
    "\n",
    "# model_ = model_hf.to('mps')\n",
    "\n",
    "# tokens = tokens.to(\"mps\")\n",
    "while tokens.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    # with torch.no_grad():\n",
    "    logits = model(tokens).realize().numpy() # (B, T, vocab_size)\n",
    "    # take the logits at the last position\n",
    "    logits = logits[:, -1, :] # (B, vocab_size)\n",
    "    logits = torch.from_numpy(logits)\n",
    "    # get the probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # do top-k sampling of 50 (huggingface pipeline default)\n",
    "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "    # select a token from the top-k probabilities\n",
    "    # note: multinomial does not demand the input to sum to 1\n",
    "    ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "    # gather the corresponding indices\n",
    "    xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "    # append to the sequence\n",
    "    tokens = torch.cat((torch.from_numpy(tokens.numpy()), xcol), dim=1)\n",
    "    tokens = Tensor(tokens.numpy(), requires_grad=False)\n",
    "    print('hi')\n",
    "\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = tokens[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7d9823e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <UOp METAL (1, 38) int ShapeTracker(views=(View(shape=(1, 38), strides=(0, 1), offset=0, mask=None, contiguous=True),))> on METAL with grad None>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = idx.realize()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2c01b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,\n",
       "        12716,   617,   661,  4327,   284,   892,   318,   517,   546,\n",
       "          366,   270,   338,   691,   351,  3037,   290,   407,  9061,\n",
       "         1600,   340,   338,  4414,  2723,    13,   198,   198,    40,\n",
       "        12546,    11]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e20a77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I\\'m a language model, which unfortunately some people tend to think is more about \"it\\'s only with technology and not computers\", it\\'s benefit source.\\n\\nI disagree,'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(idx.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1db7173d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m decoded = \u001b[43menc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/godshatter/nanogpt/.venv/lib/python3.13/site-packages/tiktoken/core.py:284\u001b[39m, in \u001b[36mEncoding.decode\u001b[39m\u001b[34m(self, tokens, errors)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: Sequence[\u001b[38;5;28mint\u001b[39m], errors: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    273\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Decodes a list of tokens into a string.\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_core_bpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m, errors=errors)\n",
      "\u001b[31mTypeError\u001b[39m: argument 'tokens': only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "decoded = enc.decode(idx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04878404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, a programming language. I have to read the code and I have to do the thing where they come together.\n",
      "> Hello, I'm a language model, I am able to model the semantics of types. All that requires is that the programmer know what values there are\n",
      "> Hello, I'm a language model, not a model of writing. When I started writing Ruby on Rails, all my work was focused on Rails and\n",
      "> Hello, I'm a language model, a language model where languages are constantly at various points in their lives. My friends and I tend to develop language\n",
      "> Hello, I'm a language model, so there isn't anything about it that's fundamentally wrong... that's what I'm doing. But I feel\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0325cce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(logits.numpy(), logits_hf.numpy(), rtol=1e-4, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadab09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
