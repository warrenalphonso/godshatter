{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In linear networks, PDLT says that covariance for activation at some neuron\n",
    "# will be $G^{(l)}_2 = C_W^l 1/n_0 \\sum_{j=1}^{n_0} x_{j}^2\n",
    "#\n",
    "# The intralayer interaction of the magnitudes then is\n",
    "# E[(z_j z_j - G_2)(z_k z_k - G_2)] = G_4 - G_2^2 = 2(l-1)/n G_2^2\n",
    "\n",
    "# TODO: Vary l and n to see around when the binomial approximation really breaks down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we'll randomly initialize an input vector\n",
    "# Then randomly initialize the network 1_000 times and store the activation at each\n",
    "# layer. Then we'll compute the covariances of these activations\n",
    "\n",
    "input_dim = 10\n",
    "# 2 inputs\n",
    "n_inputs = 2\n",
    "input = torch.randn((n_inputs, input_dim))\n",
    "input /= input.norm(dim=1, keepdim=True)\n",
    "\n",
    "attempts = 1000\n",
    "n = 1000\n",
    "L = 3\n",
    "C_W = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[0].norm(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(n, L, C_W):\n",
    "    layers = []\n",
    "    for i in range(l):\n",
    "        first_dim = input_dim if i == 0 else n\n",
    "        layer = torch.nn.Linear(first_dim, n)\n",
    "        # initialize layer\n",
    "        torch.nn.init.normal_(layer.weight, std=(C_W / first_dim) ** 0.5)\n",
    "        torch.nn.init.zeros_(layer.bias)\n",
    "        layers.append(layer)\n",
    "\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 55.45it/s]\n"
     ]
    }
   ],
   "source": [
    "activations_per_layer = defaultdict(list)\n",
    "for attempt in tqdm(range(attempts)):\n",
    "    x = input\n",
    "    model = initialize_model(n, L, C_W)\n",
    "\n",
    "    # Forward pass and compute covariance (passing layer by layer to avoid using hooks)\n",
    "    for i, layer in enumerate(model):\n",
    "        x = layer(x)\n",
    "        activations_per_layer[i].append(x.detach())\n",
    "\n",
    "for i, _ in enumerate(model):\n",
    "    activations_per_layer[i] = torch.stack(activations_per_layer[i])\n",
    "    assert activations_per_layer[i].shape == (attempts, n_inputs, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1000)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(activations_per_layer), len(activations_per_layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(sample_1, sample_2, activations):\n",
    "    # Covariance of activations in some layer\n",
    "    act_a, act_b = (\n",
    "        activations[:, sample_1, :],\n",
    "        activations[:, sample_2, :],\n",
    "    )  # 2 (attempts, 1, neurons)\n",
    "    act_a, act_b = act_a.squeeze(1), act_b.squeeze(1)  # 2 (attempts, neurons)\n",
    "    # Now do E[(X-E[X]) (Y-E[Y])]\n",
    "    act_a = act_a - act_a.mean(dim=0, keepdim=True)\n",
    "    act_b = act_b - act_b.mean(dim=0, keepdim=True)\n",
    "    cov = (act_a * act_b).mean(dim=0)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariances = {}\n",
    "\n",
    "for sample_1, sample_2 in itertools.product((0, 1), repeat=2):\n",
    "    covariances[(sample_1, sample_2)] = {}\n",
    "    for layer, activations in activations_per_layer.items():\n",
    "        cov = covariance(sample_1, sample_2, activations)\n",
    "        covariances[(sample_1, sample_2)][layer] = cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000149011612"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inner_product(sample_1, sample_2, input):\n",
    "    # return ((input[sample_1] @ input[sample_2]) / input.shape[1]).item()\n",
    "    return (\n",
    "        sum(i1 * i2 for i1, i2 in zip(input[sample_1], input[sample_2])) / input.shape[1]\n",
    "    ).item()\n",
    "\n",
    "\n",
    "inner_product(0, 0, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 114729.1015625, 0.10000000149011612, 1147290.9985290319)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If C_W was small, we expect covariance in output to be tiny always\n",
    "samples = (1, 0)\n",
    "max_ = max(abs(covariances[samples][L - 1])).item()\n",
    "ip = inner_product(*samples, input)\n",
    "C_W, max_, ip, max_ / ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=0 maximum_deviation=0.0013306538234676723\n",
      "layer=1 maximum_deviation=1.4527258799732309e-05\n",
      "layer=2 maximum_deviation=1.5048692425172388e-07\n"
     ]
    }
   ],
   "source": [
    "# Off-diagonal ones should be 0\n",
    "for layer, cov in enumerate(covariances):\n",
    "    maximum_deviation = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                var = cov[i][j]\n",
    "                if var > maximum_deviation:\n",
    "                    maximum_deviation = var\n",
    "    print(f\"{layer=} {maximum_deviation=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8131,  0.6417,  0.1147, -1.4479, -0.6727,  0.7635,  1.6740,  0.0495,\n",
       "        -0.0669,  0.2204])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7075738648175647"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's inner product of input?\n",
    "inner_product = sum(component**2 for component in input[0].numpy().flatten()) / len(\n",
    "    input[0].numpy().flatten()\n",
    ")\n",
    "inner_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=0 maximum_deviation=0.8907861530065746\n",
      "layer=1 maximum_deviation=0.008907744538290272\n",
      "layer=2 maximum_deviation=8.906750944679384e-05\n"
     ]
    }
   ],
   "source": [
    "# Diagonal entries should scale with C_W\n",
    "for layer, cov in enumerate(covariances):\n",
    "    maximum_deviation = 0\n",
    "    for i in range(n):\n",
    "        abs_deviation = abs(cov[i][i] - C_W**layer * inner_product)\n",
    "        if abs_deviation > maximum_deviation:\n",
    "            maximum_deviation = abs_deviation\n",
    "    print(f\"{layer=} {maximum_deviation=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=0\n",
      "38.866870436205375 115 538\n",
      "layer=1\n",
      "21.8250891684642 125 766\n",
      "layer=2\n",
      "12.861546160345714 4 842\n"
     ]
    }
   ],
   "source": [
    "# Now let's try the fourth cumulant\n",
    "# For every pair of activations per layer, subtract the covariance of each activation\n",
    "# and get product of deviations\n",
    "fourth_cumulant = []\n",
    "for layer in range(l):\n",
    "    print(f\"{layer=}\")\n",
    "    cov = covariances[layer]\n",
    "    fourth_cumulant.append([])\n",
    "    # Let's just use the first attempt\n",
    "    activations = activations_per_layer[layer][0]\n",
    "    # Choose two disjoint neurons\n",
    "    maximum_ = 0\n",
    "    i_, j_ = (0, 0)\n",
    "    for i in range(n):\n",
    "        fourth_cumulant[layer].append([])\n",
    "        for j in range(n):\n",
    "            deviation_i = activations[i] ** 2 - cov[i][i]\n",
    "            deviation_j = activations[j] ** 2 - cov[j][j]\n",
    "            prod = deviation_i * deviation_j\n",
    "            if prod > maximum_ and i != j:\n",
    "                maximum_ = prod\n",
    "                i_, j_ = i, j\n",
    "\n",
    "            fourth_cumulant[layer][i].append(prod)\n",
    "    print(maximum_, i_, j_)\n",
    "\n",
    "    fourth_cumulant[layer] = np.array(fourth_cumulant[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_cumulant[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=0\n",
      "layer=0 mean_deviation=-0.0003021667286407805 -0.00031977996299061256\n",
      "layer=1\n",
      "layer=1 mean_deviation=7.721738728385593e-05 0.0\n",
      "layer=2\n",
      "layer=2 mean_deviation=0.00023255435845350234 0.00031977996299061256\n"
     ]
    }
   ],
   "source": [
    "# Off-diagonal ones should be 2(l-1)/n * G_2^2\n",
    "for layer in range(l):\n",
    "    print(f\"{layer=}\")\n",
    "    cov = covariances[layer]\n",
    "    deviations_products = fourth_cumulant[layer]\n",
    "\n",
    "    expected_correlation = 2 * (layer - 1) / n * (C_W**layer * inner_product) ** 2\n",
    "\n",
    "    total_deviations_sum = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                total_deviations_sum += deviations_products[i][j]\n",
    "    mean_deviation = total_deviations_sum / (n**2 - n)\n",
    "    print(f\"{layer=} {mean_deviation=} {expected_correlation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
