{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In linear networks, PDLT says that covariance for activation at some neuron\n",
    "# will be $G^{(l)}_2 = C_W^l 1/n_0 \\sum_{j=1}^{n_0} x_{j}^2\n",
    "#\n",
    "# The intralayer interaction of the magnitudes then is\n",
    "# E[(z_j z_j - G_2)(z_k z_k - G_2)] = G_4 - G_2^2 = 2(l-1)/n G_2^2\n",
    "\n",
    "# TODO: Vary l and n to see around when the binomial approximation really breaks down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we'll randomly initialize an input vector\n",
    "# Then randomly initialize the network 1_000 times and store the activation at each\n",
    "# layer. Then we'll compute the covariances of these activations\n",
    "\n",
    "input_dim = 10\n",
    "# Random input\n",
    "input = torch.randn((1, input_dim))\n",
    "\n",
    "attempts = 1_000\n",
    "\n",
    "activations_per_layer = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "l = 3\n",
    "C_W = 1\n",
    "\n",
    "for attempt in range(attempts):\n",
    "    if attempt % 10 == 0:\n",
    "        print(attempt)\n",
    "    x = input\n",
    "    layers = []\n",
    "    for i in range(l):\n",
    "        first_dim = input_dim if i == 0 else n\n",
    "        layer = torch.nn.Linear(first_dim, n)\n",
    "        # initialize layer\n",
    "        torch.nn.init.normal_(layer.weight, std=(C_W / first_dim) ** 0.5)\n",
    "        torch.nn.init.zeros_(layer.bias)\n",
    "        layers.append(layer)\n",
    "\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    # Forward pass and compute covariance (passing layer by layer to avoid using hooks)\n",
    "    for i, layer in enumerate(model):\n",
    "        x = layer(x)\n",
    "        activations_per_layer[i].append(x.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1000)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(activations_per_layer), len(activations_per_layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance of activations per layer\n",
    "covariances = []\n",
    "for layer, activations in activations_per_layer.items():\n",
    "    arr = np.array(activations)\n",
    "    assert arr.shape == (attempts, n)  # cols are different variables, rows are observatiosn\n",
    "    cov = np.cov(arr, rowvar=False)\n",
    "    covariances.append(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=0 maximum_deviation=0.0610354025876653\n",
      "layer=1 maximum_deviation=0.059889834126678164\n",
      "layer=2 maximum_deviation=0.05731890901522189\n"
     ]
    }
   ],
   "source": [
    "# Off-diagonal ones should be 0\n",
    "for layer, cov in enumerate(covariances):\n",
    "    maximum_deviation = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                var = cov[i][j]\n",
    "                if var > maximum_deviation:\n",
    "                    maximum_deviation = var\n",
    "    print(f\"{layer=} {maximum_deviation=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39986245322023706"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's inner product of input?\n",
    "inner_product = sum(component**2 for component in input.numpy().flatten()) / len(\n",
    "    input.numpy().flatten()\n",
    ")\n",
    "inner_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=0 maximum_deviation=0.06427837598920177\n",
      "layer=1 maximum_deviation=0.059946861309742694\n",
      "layer=2 maximum_deviation=0.06606357322730416\n"
     ]
    }
   ],
   "source": [
    "# Diagonal entries should scale with C_W\n",
    "for layer, cov in enumerate(covariances):\n",
    "    maximum_deviation = 0\n",
    "    for i in range(n):\n",
    "        abs_deviation = abs(cov[i][i] - C_W**layer * inner_product)\n",
    "        if abs_deviation > maximum_deviation:\n",
    "            maximum_deviation = abs_deviation\n",
    "    print(f\"{layer=} {maximum_deviation=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=0\n",
      "38.866870436205375 115 538\n",
      "layer=1\n",
      "21.8250891684642 125 766\n",
      "layer=2\n",
      "12.861546160345714 4 842\n"
     ]
    }
   ],
   "source": [
    "# Now let's try the fourth cumulant\n",
    "# For every pair of activations per layer, subtract the covariance of each activation\n",
    "# and get product of deviations\n",
    "fourth_cumulant = []\n",
    "for layer in range(l):\n",
    "    print(f\"{layer=}\")\n",
    "    cov = covariances[layer]\n",
    "    fourth_cumulant.append([])\n",
    "    # Let's just use the first attempt\n",
    "    activations = activations_per_layer[layer][0]\n",
    "    # Choose two disjoint neurons\n",
    "    maximum_ = 0\n",
    "    i_, j_ = (0, 0)\n",
    "    for i in range(n):\n",
    "        fourth_cumulant[layer].append([])\n",
    "        for j in range(n):\n",
    "            deviation_i = activations[i] ** 2 - cov[i][i]\n",
    "            deviation_j = activations[j] ** 2 - cov[j][j]\n",
    "            prod = deviation_i * deviation_j\n",
    "            if prod > maximum_ and i != j:\n",
    "                maximum_ = prod\n",
    "                i_, j_ = i, j\n",
    "\n",
    "            fourth_cumulant[layer][i].append(prod)\n",
    "    print(maximum_, i_, j_)\n",
    "\n",
    "    fourth_cumulant[layer] = np.array(fourth_cumulant[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_cumulant[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=0\n",
      "layer=0 mean_deviation=-0.0003021667286407805 -0.00031977996299061256\n",
      "layer=1\n",
      "layer=1 mean_deviation=7.721738728385593e-05 0.0\n",
      "layer=2\n",
      "layer=2 mean_deviation=0.00023255435845350234 0.00031977996299061256\n"
     ]
    }
   ],
   "source": [
    "# Off-diagonal ones should be 2(l-1)/n * G_2^2\n",
    "for layer in range(l):\n",
    "    print(f\"{layer=}\")\n",
    "    cov = covariances[layer]\n",
    "    deviations_products = fourth_cumulant[layer]\n",
    "\n",
    "    expected_correlation = 2 * (layer - 1) / n * (C_W**layer * inner_product) ** 2\n",
    "\n",
    "    total_deviations_sum = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                total_deviations_sum += deviations_products[i][j]\n",
    "    mean_deviation = total_deviations_sum / (n**2 - n)\n",
    "    print(f\"{layer=} {mean_deviation=} {expected_correlation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
